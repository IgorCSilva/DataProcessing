# Data-Ingestion Pipeline with Broadway

## Processing Ticketing Events

Create project:
`mix new tickets --sup`

Inside the project folder, add:
- in tickets/docker-compose.yaml:
```yaml
version: "3.2"
services:
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: 'rabbitmq'
    ports:
        - 5672:5672
        - 15672:15672
    volumes:
        - ~/.docker-conf/rabbitmq/data/:/var/lib/rabbitmq/
        - ~/.docker-conf/rabbitmq/log/:/var/log/rabbitmq
    networks:
        - rabbitmq_go_net

networks:
  rabbitmq_go_net:
```

Run the RabbitMQ with `docker-compose up`.
Access the management page with `http://localhost:15672` and use username and password `guest`.

We need to add broadway as a dependency. We also need the Broadway RabbitMQ producer, which is published as a separate package.

- in tickets/mix.exs:
```elixir
  defp deps do
    [
      {:broadway, "~> 0.6"},
      {:broadway_rabbitmq, "~> 0.6"}
    ]
  end
```

At the time of writing, Broadway officially supports the following message brokers:

- Amazon SQS via broadway_sqs.
- Apache Kafka via broadway_kafka.
- Google Cloud Pub/Sub via broadway_cloud_pub_sub.
- RabbitMQ via broadway_rabbitmq.

You can also find unofficial packages for Broadway on the Hex registry, which are prefixed with off_broadway_ , for example off_broadway_redis for Redis.

Next, let’s add :lager to extra_applications in mix.exs . This is an Erlang package used by one of the broadway_rabbitmq dependencies. You have to add it before the :logger application, otherwise you might see some harmless error messages in your logs:

- in tickets/mix.exs:
```elixir
  def application do
    [
      extra_applications: [:lager, :logger],
      mod: {Tickets.Application, []}
    ]
  end
```

This is specific to broadway_rabbitmq , so if you’re using a different producer, you can skip this step.

Run `mix deps.get`.

## Broadway Callbacks In Depth

There are four callbacks available for us to implement Broadway behaviour:

- handle_message/3
- prepare_messages/2
- handle_failed/2
- handle_batch/4

- in tickets/lib/bookings_pipeline.ex:
```elixir
defmodule BookingsPipeline do
  use Broadway
  
  @producer BroadwayRabbitMQ.Producer
  
  @producer_config [
    queue: "bookings_queue",
    declare: [durable: true],
    on_failure: :reject_and_requeue
  ]
  
  def start_link(_args) do
    options = [
      name: BookingsPipeline,
      producer: [
        module: {@producer, @producer_config}
      ],
      processors: [
        default: []
      ]
    ]
    
    Broadway.start_link(__MODULE__, options)
  end
end
```

The following configuration keys are required:

- :name - which Broadway uses as a prefix when naming processes;
- :producer - which contains configuration about the source of events;
- :processors - which allows us to configure the stage processes that receive the messages and do most of the work.

Inside :producer , we must set the :module that will ask for messages. This module will manage the connection to the message broker for us.

The :declare option will create the queue bookings_queue in RabbitMQ if it doesn’t exist already. Setting this to durable: true will persist the queue between broker restarts. Finally, we set the :on_failure setting to send failed messages back to the queue.

The @producer_config settings we use are specific to RabbitMQ, so if you’re using a different message broker and producer, you must check its documentation.

Since we’re happy with the defaults, there is no need to add the :concurrency keys for BookingsPipeline . You can also set :min_demand and :max_demand in the :default keyword list, but again, we’ll stick to their default values of 5 and 10.

You can configure like:
```elixir
producer: [
  module: {@producer, @producer_config}
  concurrency: 1
],
processors: [
  default: [
    concurrency: System.schedulers_online() * 2
  ]
]
```

Using this configuration, we will end up with a data-ingestion pipeline that works like below, assuming you have two logical cores available:

Bookings Producer -> Processor 1
                  -> Processor 2
                  -> Processor 3
                  -> Processor 4

You can learn more about the underlying GenStage pipeline generated by Broadway here (https://hexdocs.pm/broadway/architecture.html)


Now,
- in tickets/lib/tickets/application.ex:
```elixir
    children = [
      BookingsPipeline
    ]
```

### Implementing handle_message/3

Code written in handle_message/3 is executed by a processor.

There are three arguments given to handle_message/3:

- The current processor group, which is the atom :default for now;
- The message itself, in the form of a %Broadway.Message{} struct;
- The context value, which is optional, and available to set in start_link/2.

The callback must return a %Broadway.Message{} struct.

- in tickets/lib/bookings_pipeline.ex:
```elixir
  def handle_message(_processor, message, _context) do
    # Add your business logic here...
    IO.inspect(message, label: "Message")
  end
```

With RabbitMQ up, run application:
`iex -S mix`

You will see something like this:
```sh
...
21:45:02.708 [info] Application rabbit_common started on node nonode@nohost
21:45:02.718 [info] Application amqp_client started on node nonode@nohost
21:45:02.718 [info] Application amqp started on node nonode@nohost
21:45:02.718 [info] Application nimble_options started on node nonode@nohost
21:45:02.718 [info] Application broadway_rabbitmq started on node nonode@nohost
...
```

Now let’s go to http://localhost:15672/#/queues.
The “Queues” tab shows the list of all queues, and you can see that bookings_queue has been created automatically.

Click on “bookings_queue” to go to this queue’s dashboard.
Scroll down to the section “Publish message” and click on it to expand it.

Enter `musical,1` in the payload field, and click the “Publish message” submit button.

In IEx shell you must see:
```sh
iex(1)> Message: %Broadway.Message{
  data: "musical,1",
  metadata: %{
    amqp_channel: %AMQP.Channel{
      conn: %AMQP.Connection{pid: #PID<0.1034.0>},
      pid: #PID<0.1045.0>,
      custom_consumer: nil
    }
  },
  acknowledger: {BroadwayRabbitMQ.Producer,
   %AMQP.Channel{
     conn: %AMQP.Connection{pid: #PID<0.1034.0>},
     pid: #PID<0.1045.0>,
     custom_consumer: nil
   },
   %{
     client: BroadwayRabbitMQ.AmqpClient,
     delivery_tag: 1,
     on_failure: :reject_and_requeue,
     on_success: :ack,
     redelivered: false
   }},
  batcher: :default,
  batch_key: :default,
  batch_mode: :bulk,
  status: :ok
}
```

- in tickets/lib/tickets.ex:
```elixir
  def tickets_available?(_event) do
    Process.sleep(Enum.random(100..200))
    true
  end

  def create_ticket(_user, _event) do
    Process.sleep(Enum.random(250..1000))
  end

  def send_email(_user) do
    Process.sleep(Enum.random(100..20))
  end

  @users [
    %{id: "1", email: "foo@email.com"},
    %{id: "2", email: "bar@email.com"},
    %{id: "3", email: "baz@email.com"}
  ]

  def users_by_ids(ids) when is_list(ids) do
    # Database query selecting users by ids.
    Enum.filter(@users, & &1.id in ids)
  end
```

Before we send the confirmation email, we can fetch the user from the database, and pass it to send_email/1 . However, if you do this in handle_message/3 , there’ll be a database request being made for each incoming message. This isn’t ideal, especially if we’re handling a lot of messages in a real production application.

When it comes to preloading data, the prepare_messages/2 callback is here to help.

### Implementing prepare_messages/2

The prepare_messages/2 callback runs before handle_message/3 and receives a list of messages. Use prepare_messages/2 only for
preloading data.

- in tickets/lib/bookings_pipeline.ex:
```elixir
  def prepare_messages(messages, _context) do
    # Parse data and convert to a map.
    messages =
      Enum.map(messages, fn message ->
        Broadway.Message.update_data(message, fn data ->
          [event, user_id] = String.split(data, ",")
          %{event: event, user_id: user_id}
        end)
      end)
      
    users = Tickets.users_by_ids(Enum.map(messages, & &1.data.user_id))
    
    # Put users in messages.
    Enum.map(messages, fn message ->
      Broadway.Message.update_data(message, fn data ->
        user = Enum.find(users, & &1.id == data.user_id)
        Map.put(data, :user, user)
      end)
    end)
  end

  def handle_message(_processor, message, _context) do
    %{data: %{event: event, user: user}} = message
    
    # TODO: check for tickets availability.
    
    Tickets.create_ticket(user, event)
    Tickets.send_email(user)
    
    IO.inspect(message, label: "Message")
  end
```

Let’s restart the application, and head over to the RabbitMQ management panel.

If you publish the same message as before — musical,1 — you will get a slightly different output this time:

```sh
iex(1)> Message: %Broadway.Message{
  data: %{
    event: "musical",
    user: %{email: "foo@email.com", id: "1"},
    user_id: "1"
  },
  ...
}
```

- Caution When Using prepare_messages/2

You should always keep your main business logic in handle_message/3 and use prepare_messages/2 only for preloading data. Also be careful with prepare_messages/2 . A potential error in your code will cause all messages to be marked as failed.

### Implementing handle_failed/3

If an unhandled exception happens in your business logic, the message will be marked as failed. You can also manually mark a message as failed using Broadway.Message.failed/2. This could be useful when you want to discard messages that you don’t want to process.

Remember that in @producer_config we configured the :on_failure setting with the value :reject_and_requeue . This means that failed messages will be sent back to RabbitMQ and redelivered. By default, messages will be redelivered indefinitely. We can change this setting to :reject_and_requeue_once or simply :reject to completely discard all failed messages.


Return false for cinema event.
- in tickets/lib/tickets.ex:
```elixir
  def tickets_available?("cinema") do
    Process.sleep(Enum.random(100..200))
    false
  end
```

- in tickets/lib/bookings_pipeline.ex:
```elixir
  def handle_message(_processor, message, _context) do
    %{data: %{event: event, user: user}} = message

    if Tickets.tickets_available?(event) do
      Tickets.create_ticket(user, event)
      Tickets.send_email(user)
      IO.inspect(message, label: "Message")
    else
      Broadway.Message.failed(message, "bookings-closed")
    end
  end
```

Broadway.Message.failed/2
lets you specify the reason why the message has failed. This is useful because it helps you to decide what to do with the message later.

- in tickets/lib/bookings_pipeline.ex:
```elixir
  
  def handle_failed(messages, _context) do
    IO.inspect(messages, label: "Failed messages")
    
    Enum.map(messages, fn
      %{status: {:failed, "bookings-closed"}} = message ->
      Broadway.Message.configure_ack(message, on_failure: :reject)
      
      message -> message
    end)
  end
```


When a message fails with the reason
"bookings-closed" , we will discard it, otherwise we will keep retrying. We use the Broadway.Message.configure_ack/2 helper to overwrite the acknowledgment setting for the message.

Restart the application and try publishing a message with payload `cinema,2` . You should see this in IEx:

```sh
iex(1)> Failed messages: [
  %Broadway.Message{
    data: %{
      event: "cinema",
      user: %{email: "bar@email.com", id: "2"},
      user_id: "2"
    },
    ...
    status: {:failed, "bookings-closed"}
  }
]

```

When a message fails due an error, the :status field value will be {:error, error, stacktrace}.

- Configuring RabbitMQ Queues
If you use the :reject_and_requeue setting, messages will be redelivered indefinitely. You can put a limit on retries by configuring the time to live (TTL) for messages using a policy, or use a dead-letter queue. You can check the RabbitMQ documentation for more information.

## Batching Messages

- in tickets/mix.exs:
```elixir
  defp deps do
    [
      {:broadway, "~> 0.6"},
      {:broadway_rabbitmq, "~> 0.6"},
      {:amqp, "~> 1.6"}
    ]
  end
```

Run `mix deps.get`.


- in tickets/.iex.exs:
```elixir
send_message = fn num_messages ->

  {:ok, connection} = AMQP.Connection.open()
  {:ok, channel} = AMQP.Channel.open(connection)

  Enum.each(1..num_messages, fn _ ->
    event = Enum.random(["cinema", "musical", "play"])
    user_id = Enum.random(1..3)
    AMQP.Basic.publish(channel, "", "bookings_queue", "#{event},#{user_id}")
  end)

  AMQP.Connection.close(connection)
end
```

Now, remove the code added in tickets/lib/tickets.ex:
```elixir
  def tickets_available?("cinema") do
    Process.sleep(Enum.random(100..200))
    false
  end
```

Restart the application and execute:
```sh
ex(1)> send_message.(2)
:ok
iex(2)> Message: %Broadway.Message{
  data: %{
    event: "play",
    user: %{email: "bar@email.com", id: "2"},
    user_id: "2"},
  ...
}
Message: %Broadway.Message{
  data: %{
    event: "musical",
    user: %{email: "baz@email.com", id: "3"},
    user_id: "3"
  },
  ...
}
```

### Implementing Batching and handle_batch/4
Just like handle_message/3 , handle_batch/4 is special, and all code within the callback runs concurrently in a separate batch processor.

- in tickets/lib/bookings_pipeline.ex:
```elixir
  def start_link(_args) do
    options = [
      name: BookingsPipeline,
      producer: [
        module: {@producer, @producer_config}
      ],
      processors: [
        default: []
      ],
      batchers: [
        default: []
      ]
    ]

    Broadway.start_link(__MODULE__, options)
  end

  ...

  def handle_batch(_batcher, messages, batch_info, _context) do
    IO.inspect(batch_info, label: "#{inspect(self())} Batch")
    
    messages
  end
```

Adding the :batchers key will instruct Broadway to extend the pipeline.

The handle_batch/4 callback receives the current batcher (which will be :default in this case) and the list of messages in the batch.

You can still mark some messages as failed at this stage, otherwise Broadway will automatically acknowledge all of them as successful.

Restart the application and send more messages:
```sh
iex(1)> send_message.(500)
:ok
iex(2)>
...
#PID<0.352.0> Batch: %Broadway.BatchInfo{
  batcher: :default,
  batch_key: :default,
  partition: nil,
  size: 15
}
...
#PID<0.352.0> Batch: %Broadway.BatchInfo{
  batcher: :default,
  batch_key: :default,
  partition: nil,
  size: 7
}
...
#PID<0.352.0> Batch: %Broadway.BatchInfo{
  batcher: :default,
  batch_key: :default,
  partition: nil,
  size: 1
}
```

The %BatchInfo{} fields:
- :batcher - is the batcher group and belongs to one of the groups defined in start_link/1.

- :batch_key - is an identifier for a group of a messages within the batch.

- :partition - is the partition key, if partitioning is configured (this is an optional feature).

- :size - is the number of messages in the batch.


### Using Static Batching
Each message returned by the handle_message/3 callback must belong to a batcher. If one hasn’t been assigned, messages will go to the :default batcher.

- in tickets/lib/bookings_pipeline.ex:
```elixir
  def start_link(_args) do
    options = [
      name: BookingsPipeline,
      producer: [
        module: {@producer, @producer_config}
      ],
      processors: [
        default: []
      ],
      batchers: [
        cinema: [],
        musical: [],
        default: []
      ]
    ]

    Broadway.start_link(__MODULE__, options)
  end

  ...

  def handle_message(_processor, message, _context) do
    if Tickets.tickets_available?(message.data.event) do
      case message do
        %{data: %{event: "cinema"}} = message ->
          Broadway.Message.put_batcher(message, :cinema)
          
        %{data: %{event: "musical"}} = message ->
          Broadway.Message.put_batcher(message, :musical)
          
        message -> message
          
      end
    else
      Broadway.Message.failed(message, "bookings-closed")
    end
  end

  ...

  def handle_batch(_batcher, messages, batch_info, _context) do
    IO.puts("#{inspect(self())} Batch #{batch_info.batcher} #{batch_info.batch_key}")
    
    messages
    |> Tickets.insert_all_tickets()
    |> Enum.each(fn %{data: %{user: user}} ->
      Tickets.send_email(user)
    end)
    
    messages
  end
```

- in tickets/lib/tickets.ex:
```elixir
  def insert_all_tickets(messages) do
    Process.sleep(Enum.count(messages) * 250)
    messages
  end
```

Restart the application and send messages again:
```sh
iex(6)> send_message.(500)
:ok
iex(7)> #PID<0.345.0> Batch musical default 
#PID<0.341.0> Batch cinema default
#PID<0.349.0> Batch default default
#PID<0.349.0> Batch default default
#PID<0.341.0> Batch cinema default
#PID<0.345.0> Batch musical default
...
```

By default, you get one batch processor per batcher, but you can increase this by using the :concurrency key on each batcher group.

You can execute different logic per batcher, like:
```elixir
  def handle_batch(:cinema, messages, batch_info, _context) do
    ...
  end

  def handle_batch(_batcher, messages, batch_info, _context) do
    ...
  end
```

### Using Dynamic Batching
To use dynamic batching, you can set :batch_key on the message using Broadway.Message.put_batch_key/2 in your handle_message/3 callback. The batch key is then used to partition the data within each batch for each batcher group.

- in tickets/lib/notifications_pipeline.ex:
```elixir
defmodule NotificationsPipeline do
  use Broadway

  @producer BroadwayRabbitMQ.Producer

  @producer_config [
    queue: "notifications_queue",
    declare: [durable: true],
    on_failure: :reject_and_requeue,
    qos: [prefetch_count: 100]
  ]

  def start_link(_args) do
    options = [
      name: NotificationsPipeline,
      producer: [
        module: {@producer, @producer_config}
      ],
      processors: [
        default: []
      ],
      batchers: [
        email: [concurrency: 5, batch_timeout: 10_000]
      ]
    ]

    Broadway.start_link(__MODULE__, options)
  end

  def prepare_messages(messages, _context) do
    Enum.map(messages, fn message ->
      Broadway.Message.update_data(message, fn data ->
        [type, recipient] = String.split(data, ",")
        %{type: type, recipient: recipient}
      end)
    end)
  end

  def handle_message(_processor, message, _context) do
    message
    |> Broadway.Message.put_batcher(:email)
    |> Broadway.Message.put_batch_key(message.data.recipient)
  end

  def handle_batch(_batcher, messages, batch_info, _context) do
    IO.puts("#{inspect(self())} Batch #{batch_info.batcher} #{batch_info.batch_key}")

    # Send an email digest to the user with all information.

    messages
  end
end
```

With batch_timeout we’re introducing a window of ten seconds for sending an email to a recipient.

- in tickets/lib/tickets/application.ex:
```elixir
    children = [
      BookingsPipeline,
      NotificationsPipeline
    ]
```

- in tickets/lib/bookings_pipeline.ex:
```elixir
  def handle_batch(_batcher, messages, batch_info, _context) do
    IO.puts("#{inspect(self())} Batch #{batch_info.batcher} #{batch_info.batch_key}")

    messages
    |> Tickets.insert_all_tickets()
    |> Enum.each(fn message ->
      channel = message.metadata.amqp_channel
      payload = "email,#{message.data.user.email}"
      AMQP.Basic.publish(channel, "", "notifications_queue", payload)
    end)

    messages
  end
```

Restart the application.
```sh
iex(1)> send_message.(200)
:ok
iex(2)> #PID<0.368.0> Batch musical default
#PID<0.372.0> Batch default default
#PID<0.364.0> Batch cinema default
#PID<0.372.0> Batch default default
#PID<0.368.0> Batch musical default
#PID<0.364.0> Batch cinema default
#PID<0.368.0> Batch musical default
#PID<0.372.0> Batch default default
#PID<0.364.0> Batch cinema default
#PID<0.368.0> Batch musical default
#PID<0.372.0> Batch default default
#PID<0.401.0> Batch email bar@email.com
#PID<0.405.0> Batch email foo@email.com
#PID<0.402.0> Batch email baz@email.com
#PID<0.368.0> Batch musical default
#PID<0.364.0> Batch cinema default
#PID<0.372.0> Batch default default
#PID<0.368.0> Batch musical default
#PID<0.409.0> Batch email bar@email.com
#PID<0.408.0> Batch email baz@email.com
#PID<0.405.0> Batch email foo@email.com
```

Watching the output, you’ll see emails being sent, at most, only once every ten seconds.

### Adjusting Batch Size and Timeout

The maximum number of received messages is also known as batch size and is typically controlled by the :batch_size value for each batcher in start_link/1, like so:

```elixir
...
batchers: [
  ...
  default: [
    batch_size: 100
  ]
]
```

The default value for :batch_size for each batcher is already 100. It may seem like something is going wrong, since we’re only getting half the expected amount.

This is actually by design and is specific to the broadway_rabbitmq producer. The BroadwayRabbitMQ.Producer module maintains an active connection to the RabbitMQ server, which allows it to receive messages as soon as they get in the queue. This is why it sets a limit on the number of messages coming from RabbitMQ, so it can control the flow and handle back-pressure. This is documented in the official broadway_rabbitmq documentation page.

The good news is that you can easily increase this limit by passing some extra configuration to your producer config with prefetch_count field:

```elixir
@producer_config [
  queue: "bookings_queue",
  declare: [durable: true],
  on_failure: :reject_and_requeue,
>> qos: [prefetch_count: 100]
]
```

Here we set :prefetch_count to 100, which is the same as the default value for :batch_size.

Now you can tweak :batch_size per batcher, and each will be able to receive up to 100 messages:

```elixir
batchers: [
  cinema: [batch_size: 75],
  musical: [], # defaults to :batch_size of 100
  default: [batch_size: 50]
]
```

## Using GenStage Producers
We already used GenStage and Flow to implement scraper pipeline logic, so it will be great to have another version implemented using Broadway, for comparison.

- in scraper/mix.exs:
```elixir
  # Run "mix help deps" to learn about dependencies.
  defp deps do
    [
      {:gen_stage, "~> 1.0"},
      {:flow, "~> 1.0"},
      {:broadway, "~> 0.6"}
    ]
  end
```

Run `mix deps.get`.

- in scraper/lib/scrapingPipeline.ex:
```elixir
defmodule ScrapingPipeline do
  use Broadway
  require Logger

  def start_link(_args) do
    options = [
      name: ScrapingPipeline,
      producer: [
        module: {PageProducer, []},
        transformer: {ScrapingPipeline, :transform, []}
      ],
      processors: [
        default: []
      ]
    ]

    Broadway.start_link(__MODULE__, options)
  end

  def transform(event, _options) do
    # :pages is the acknowledger id.
    %Broadway.Message{
      data: event,
      acknowledger: {ScrapingPipeline, :pages, []}
    }
  end

  # :pages - the acknowledger id
  # successful - list of successful messages
  # failed - list of failed messages
  def ack(:pages, _successful, _failed) do
    :ok
  end
end

```

Broadway will use the producer module configuration to start PageProducer as part of the pipeline.

The acknowledger receives groups of messages that have been
processed, successfully or not. This is usually an opportunity to contact the message broker and inform it of the outcome. But in this case, PageProducer doesn’t really care if a message has been processed or not. That’s why we can implement the acknowledger like ack function above.

Right now we just return :ok regardless of the outcome for each message. Perhaps in the future, if PageProducer has an internal queue, we can send messages back to be retried.

Update application.ex file to use pipeline.
- in scraper/lib/scraper/application.ex:
```elixir
    children = [
      ScrapingPipeline
    ]
```

### Updating PageProducer
Since ScrapingPipeline is starting the process for us, we no longer need the start_link/2 function in PageProducer , so we can delete it.

Update scrape_pages function.
- in scraper/lib/page_producer.ex:
```elixir
  def scrape_pages(pages) when is_list(pages) do
    ScrapingPipeline
    |> Broadway.producer_names()
    |> List.first()
    |> GenStage.cast({:pages, pages})
  end
```

### Configuring ScrapingPipeline

- in scraper/lib/scrapingPipeline.ex:
```elixir
  def start_link(_args) do
    options = [
      name: ScrapingPipeline,
      producer: [
        module: {PageProducer, []},
        transformer: {ScrapingPipeline, :transform, []}
      ],
      processors: [
        default: [max_demand: 1, concurrency: 2]
      ],
      batchers: [
        default: [batch_size: 1, concurrency: 2]
      ]
    ]

    Broadway.start_link(__MODULE__, options)
  end
  
  def handle_message(_processor, message, _context) do
    if Scraper.online?(message.data) do
      Broadway.Message.put_batch_key(message, message.data)
    else
      Broadway.Message.failed(message, "offline")
    end
  end
  
  def handle_batch(_batcher, [message], _batch_info, _context) do
    Logger.info("Batch Processor received #{message.data}")
    
    Scraper.work()
    [message]
  end
```

Start up the application and test.
```sh
07:47:04.614 [info] PageProducer init
 
07:47:04.619 [info] PageProducer received demand for 1 pages
 
07:47:04.619 [info] PageProducer received demand for 1 pages


iex(1)> PageProducer.scrape_pages(pages)
:ok
iex(2)> 
07:47:26.704 [info] Batch Processor received google.com
 
07:47:27.705 [info] PageProducer received demand for 1 pages
 
07:47:27.705 [info] Batch Processor received apple.com
 
07:47:29.705 [info] Batch Processor received netflix.com
 
07:47:30.706 [info] PageProducer received demand for 1 pages
 
07:47:30.706 [info] Batch Processor received amazon.com
```

Broadway producers also get some extra callbacks, so they can integrate even better with the rest of the pipeline. They are prepare_for_start/2 and prepare_for_draining/2 , which run when the pipeline starts and stops, respectively. You can read more about them here. ([text](https://hexdocs.pm/broadway/Broadway.Producer.html))

## Wrapping Up

Whenever you want to perform work concurrently without much hassle, you can use the Task module. Using Task.async_stream/3 is a great way to process large collections of data and provide back-pressure at the same time.

However, complex tasks often need to maintain their own state and run over long periods of time. This is where GenServer comes in handy. You can use it to create processes that you can interact with, and solve harder problems.

Sometimes the amount of concurrent work that you have to perform is variable, and you are at risk of exhausting your system resources. For example, surges of traffic frequently bring even the largest web services offline, because they are unable to allocate resources to handle the increased traffic. GenStage enables you to create complex data-processing pipelines that are resilient by controlling consumer demand and handling back-pressure.

Flow helps you aggregate data by leveraging GenStage under the hood. If you are working with big datasets and you’re filtering, mapping, and reducing data, then this is probably the best tool for you to use.

Finally, Broadway fills an important gap when building data-ingestion pipelines that consume external events. You can use it not only with message brokers but also to build robust event-processing systems.